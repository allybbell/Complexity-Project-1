## Project 1 Draft Review

##### Question: What is your understanding of the experiment the team is replicating? What question does it answer? How clear is the team's explanation?

The team is replicating an experiment that is a variation of the Barabasi-Albert algorithm that introduces added functionality and parameters such that it is a better model for social networks. From this, they are applying this model to a Wikipedia network and comparing the differences observed.

##### Methodology: Do you understand the methodology? Does it make sense for the question? Are there limitations you see that the team did not address?

I think the summary of the approach made a lot of sense - although I wasn't entirely clear exactly how the teams algorithm was working, and what part they had added on. It may have been helpful to have some visualization of the methodology to give readers a better picture of what is being modeled.

##### Results: Do you understand what the results are (not yet considering their interpretation)? If they are presented graphically, are the visualizations effective? Do all figures have labels on the axes and captions?

I think I have a clear understanding of how the results will be presented, but the project is not there yet. Looks like its on a clear track, and the team has a plan for producing visualizations.

##### Interpretation: Does the draft report interpret the results as an answer to the motivating question? Does the argument hold water?

I'm not quite sure what the guiding question here is, outside of just comparing these two networks. It seems really interesting, and the team has done a really good job laying out the differences, although I'm not clear about what the implications of their results could be.

##### Replication: Are the results in the report consistent with the results from the original paper? If so, how did the authors demonstrate that consistency? Is it quantitative or qualitative?

Not yet, but a good job talking about how these will be compared once they're produced.

##### Extension: Does the report explain an extension to the original experiment clearly? Can it answer an interesting question that the original experiment did not answer?

Yes - the report explains the extension well, and it sounds interesting. I'm not super clear on what elements of the model they are adding on to the one presented in the paper and which are replication. Right now, it doesn't feel like this model is answering a particular question, and this could be discussed more explicitly.

##### Progress: Is the team roughly where they should be at this point, with a replication that is substantially complete and an extension that is clearly defined and either complete or nearly so?

I'm not sure, the report isn't clear about what results the team has gotten so far and how they have compared to the ones they are trying to recreate. I think the project is well scoped, and they have a clear understanding of a path forward, but I'm not sure where they are on this. The extension is clearly defined, although currently incomplete.

##### Presentation: Is the report written in clear, concise, correct language? Is it consistent with the audience and goals of the report? Does it violate any of the recommendations in my style guide?

This report was really easy to read, they did a good job of making the material digestible. I think it may be interesting to better lay out how the algorithm is working by including equations or a visual representation of this. 

##### Mechanics: Is the report in the right directory with the right file name? Is it formatted professionally in Markdown? Does it include a meaningful title and the full names of the authors? Is the bibliography in an acceptable style? 

Looks good! Maybe the title could be slightly more explicit than just "Other Networks".